# ğŸ§  LLM Playground â€“ FastAPI + Gemini + React

A sleek, minimal **LLM playground** built with:

- âš™ï¸ **FastAPI** backend
- ğŸ¤– **Google Gemini** LLM
- âš›ï¸ **React (Vite)** frontend
- ğŸ¨ **Tailwind CSS** for styling

This project is designed to look and feel like a **real production LLM client**, perfect for showcasing **AI application engineering** skills to recruiters and hiring managers.

---

## âœ¨ Features

### Backend (FastAPI + Gemini)
- âœ… `/api/v1/ask` endpoint to query the LLM
- âœ… Clean, versioned API structure (`/api/v1/...`)
- âœ… Gemini integration via `google-generativeai`
- âœ… Configurable `max_tokens` per request (up to 100,000 in the UI)
- âœ… Strong logging & error handling:
  - Centralized logging config
  - Custom `LLMServiceError`
  - Clean JSON error responses
- âœ… Environment-based configuration with `.env` (no keys in code)

### Frontend (React + Vite + Tailwind)
- âœ… Minimal, **industry-style** LLM playground UI
- âœ… Prompt textarea with helper examples
- âœ… Max Tokens slider with live token display
- âœ… Loading state (â€œThinkingâ€¦â€ with spinner)
- âœ… Response panel with:
  - Model name
  - Token usage
- âœ… â€œRecent promptsâ€ sidebar with quick reuse
- âœ… Clean bottom meta line:

  > **Built with FastAPI Â· Gemini Â· React Â· Tailwind CSS**  
  > LLM Playground Â· Industry-ready LLM client UI

---

## ğŸ§± Tech Stack

**Backend**
- Python 3.10+
- FastAPI
- Uvicorn
- `google-generativeai` (Gemini)
- `pydantic-settings` for config
- `python-dotenv` for env loading

**Frontend**
- React (Vite)
- Tailwind CSS
- PostCSS + Autoprefixer

---

## ğŸ—ï¸ Architecture & Folder Structure

```bash
.
â”œâ”€â”€ app/                     # FastAPI application package
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ routes/
â”‚   â”‚           â””â”€â”€ ask.py   # /api/v1/ask endpoint
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py        # Settings (env + Pydantic)
â”‚   â”‚   â””â”€â”€ logging_config.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py       # AskRequest, AskResponse
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â””â”€â”€ llm_client.py    # Gemini LLM client
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ errors.py        # LLMServiceError, error helpers
â”‚   â””â”€â”€ main.py              # FastAPI app factory, CORS, routers
â”œâ”€â”€ llm-frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx          # LLM playground UI
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ index.css        # Tailwind entry
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .env                     # Local env vars (NOT committed)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸš€ Getting Started
1ï¸âƒ£ Prerequisites

Python 3.10+

Node.js 16+ and npm

A Gemini API key from Google AI Studio

2ï¸âƒ£ Backend Setup (FastAPI + Gemini)

From project root:

# create virtual env (optional but recommended)
python -m venv venv
venv\Scripts\activate  # on Windows
# source venv/bin/activate  # on macOS/Linux

# install dependencies
pip install -r requirements.txt

ğŸ” Configure environment variables

Create a .env file in the project root (same level as app/):

GEMINI_API_KEY=AIzaSyXXXXXXXXXXXXXXX   # your real Gemini key


âš ï¸ Never commit .env to GitHub.
You can add .env to your .gitignore.

â–¶ï¸ Run the backend

From the project root:

uvicorn app.main:app --reload


Backend will be available at:

Swagger docs: http://127.0.0.1:8000/docs

/api/v1/ask: POST endpoint

3ï¸âƒ£ Frontend Setup (React + Vite + Tailwind)

From the llm-frontend directory:

cd llm-frontend

# install frontend dependencies
npm install

# run dev server
npm run dev


Frontend will usually be at:

http://127.0.0.1:5173
 or http://localhost:5173

The frontend expects the backend at http://127.0.0.1:8000.
CORS is enabled on the FastAPI side.

ğŸ“¡ API Reference
POST /api/v1/ask

Send a prompt to the LLM and receive a generated response.

Request body
{
  "prompt": "Explain what an API is in simple terms for a beginner.",
  "max_tokens": 300
}


prompt (string, required) â€“ userâ€™s question or instruction

max_tokens (int, optional) â€“ maximum tokens to generate (UI supports up to 100,000; backend passes this to Gemini)

Successful response
{
  "answer": "An API is like a waiter in a restaurant...",
  "model": "models/gemini-2.5-flash",
  "usage_tokens": 1234
}

Error response (example)
{
  "detail": "Gemini API error: 400 API key not valid. Please pass a valid API key."
}


Errors are handled centrally and surfaced in a clean JSON format.

ğŸ–¥ï¸ Frontend UI Overview

The React frontend provides:

ğŸ“ Prompt input with placeholder & helper examples

ğŸšï¸ Max tokens slider (50 â†’ 100,000 tokens)

ğŸ”„ Loading state (â€œThinkingâ€¦â€ + spinner)

ğŸ“¥ Response panel showing:

Answer text

Model name

Total tokens used

ğŸ•’ Recent prompts list (last 5)

Click any past prompt to re-use it instantly

â„¹ï¸ Bottom meta info for a clean portfolio touch:

Built with FastAPI Â· Gemini Â· React Â· Tailwind CSS
LLM Playground Â· Industry-ready LLM client UI
